#!/usr/bin/env bash

BASE_DIR="$(pwd)"

# ğŸ“Œ Definir subdiretÃ³rios
HTTPX_DIR="$BASE_DIR/httpx"
DIRSEARCH_DIR="$BASE_DIR/dirsearch"
URLS_DIR="$BASE_DIR/urls"
GITHUB_DIR="$BASE_DIR/github"
REPORT_FILE="$BASE_DIR/relatorio.txt"

# ğŸ“Œ Criar diretÃ³rios necessÃ¡rios
mkdir -p "$URLS_DIR" "$HTTPX_DIR" "$DIRSEARCH_DIR" "$GITHUB_DIR"

# ğŸ“Œ Verificar dependÃªncias
check_dependency() {
    command -v "$1" >/dev/null 2>&1 || { echo "âŒ Erro: $1 nÃ£o encontrado. Instale antes de continuar."; exit 1; }
}

# ğŸ“Œ Ferramentas obrigatÃ³rias
TOOLS=("waybackurls" "gau" "httpx" "gospider" "dirsearch" "hakrawler")

for tool in "${TOOLS[@]}"; do
    check_dependency "$tool"
done

# ğŸ“Œ Verificar argumento de entrada
if [ -z "$1" ]; then
    echo "âŒ Uso: $0 <arquivo-com-dominios-ou-URL-unica>"
    exit 1
fi

# ğŸ“Œ Detectar se argumento Ã© arquivo ou URL Ãºnica
if [[ -f "$1" ]]; then
    INPUT_FILE="$1"
elif [[ "$1" =~ ^https?:// ]]; then
    INPUT_FILE=$(mktemp)
    echo "$1" > "$INPUT_FILE"
else
    echo "âŒ Argumento invÃ¡lido. Deve ser uma URL vÃ¡lida ou um arquivo com URLs/domÃ­nios."
    exit 1
fi

# ğŸ“Œ FunÃ§Ã£o de log
log() {
    echo -e "[ $(date +'%Y-%m-%d %H:%M:%S') ] $1"
}

# ğŸ“Œ Gerar User-Agent aleatÃ³rio
USER_AGENT_LIST=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
                 "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
                 "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36")

RANDOM_UA="${USER_AGENT_LIST[$RANDOM % ${#USER_AGENT_LIST[@]}]}"

# ğŸ“Œ Coletar URLs
log "ğŸ” Coletando URLs..."
{
    cat "$INPUT_FILE" | waybackurls > "$URLS_DIR/waybackurls.txt" &
    grep -E '^[a-zA-Z0-9._-]+' "$INPUT_FILE" | gau --providers otx,urlscan --blacklist ttf,woff,svg,png --timeout 30 > "$URLS_DIR/gau.txt" &

    # Limpar e preparar arquivo para waymore
    temp_file=$(mktemp)
    sed -E 's|https?://||g; s|:[0-9]*$||g; s|[/?#].*$||g' "$INPUT_FILE" | grep -E '^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$' > "$temp_file"

    python3 /waymore/waymore.py -i "$temp_file" -oU "$URLS_DIR/waymore.txt" -xcc
    rm "$temp_file"

    gospider -q -d 3 -S "$INPUT_FILE" -u "$RANDOM_UA" | awk '{print $3}' | grep -v 'code-200' > "$URLS_DIR/gospider.txt" &
    cat "$INPUT_FILE" | hakrawler -d 2 > "$URLS_DIR/hakrawler.txt" &
    wait
}

cat "$INPUT_FILE" | sed -E 's|https?://||g; s|:[0-9]*$||g' | cut -d '/' -f1 | sort -u | while read -r domain; do
    github-endpoints -d "$domain" -t "$GITHUB_TOKEN" >> "$GITHUB_DIR/github-endpoints.txt"
done

# ğŸ“Œ Consolidar e remover duplicatas
log "ğŸ“Œ Removendo duplicatas de URLs..."
cat "$URLS_DIR"/*.txt | sort -u > "$URLS_DIR/unique_urls.txt"

# ğŸ“Œ Esperar entre 2 e 5 segundos
sleep $((RANDOM % 4 + 2))

# ğŸ“Œ Verificar URLs ativas com httpx
log "ğŸš€ Verificando URLs ativas com httpx..."
cat "$URLS_DIR/unique_urls.txt" | httpx -mc 200,301,302,403,500 -title -tech-detect -timeout 10 -silent -o "$HTTPX_DIR/httpx_filtered.txt"

# ğŸ“Œ Limpar caracteres ANSI
log "ğŸ“Œ Limpando saÃ­da do httpx..."
awk '{gsub(/\x1b\[[0-9;]*m/, "")}1' "$HTTPX_DIR/httpx_filtered.txt" > "$HTTPX_DIR/httpx_no_ansi.txt"

# ğŸ“Œ Preparar para o dirsearch
awk '{print $1}' "$HTTPX_DIR/httpx_no_ansi.txt" | grep -E '^https?://' > "$HTTPX_DIR/httpx_clean.txt"

# ğŸ“Œ Rodar dirsearch
if [ -s "$HTTPX_DIR/httpx_clean.txt" ]; then
    log "ğŸš€ Rodando dirsearch..."
    cat "$HTTPX_DIR/httpx_clean.txt" | xargs -P 5 -I{} dirsearch -u {} -e php,html,js,zip,txt -t 50 -x 403,404 -o "$DIRSEARCH_DIR/dirsearch.txt"
else
    log "âš  Nenhuma URL ativa encontrada. Pulando dirsearch."
fi

# ğŸ“Œ Gerar relatÃ³rio final
log "ğŸ“Š Gerando relatÃ³rio final..."
{
    echo "ğŸ“Œ RelatÃ³rio de ExecuÃ§Ã£o - $(date)"
    echo "--------------------------------------------------"
    echo "ğŸ” Total de URLs coletadas: $(wc -l < "$URLS_DIR/unique_urls.txt")"
    echo "ğŸš€ Total de URLs ativas: $(wc -l < "$HTTPX_DIR/httpx_clean.txt")"
    echo "ğŸ“‚ Resultados disponÃ­veis em: $BASE_DIR"
} > "$REPORT_FILE"

log "âœ… Finalizado! RelatÃ³rio salvo em '$REPORT_FILE'."
